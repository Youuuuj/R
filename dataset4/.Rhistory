model <- build_model()
history <- model %>% fit(
x = train_df %>% select(-label),
y = train_df$label,
epochs = 500,
validation_split = 0.2,
verbose = 0,
callbacks = list(early_stop)
)
plot(history)
# 모델평가
c(loss, mae) %<-% (model %>% evaluate(test_df %>% select(-label), test_df$label, verbose = 0))
paste0("Mean absolute error on test set: $", sprintf("%.2f", mae * 1000))
# 예측
test_predictions <- model %>% predict(test_df %>% select(-label))
test_predictions[ , 1]
model
test_predictions[ , 1]
# 예측값 실제값 비교
plot(test_predictions, test_labels)
abline(a=0,b=1,col="blue",lty=1)
train_df
test_df
qqnorm(res)
train_df_scale <- transform(train_df,
CRIM = scale(train_df$CRIM, center = T, scale = T),
ZN = scale(train_df$ZN),
INDUS = scale(train_df$INDUS),
CHAS = scale(train_df$CHAS),
NOX = scale(train_df$NOX),
RM = scale(train_df$RM),
AGE = scale(train_df$AGE),
DIS = scale(train_df$DIS),
RAD = scale(train_df$RAD),
TAX = scale(train_df$TAX),
PTRATIO = scale(train_df$PTRATIO),
B = scale(train_df$B),
LSTAT = scale(train_df$LSTAT),
label = scale(train_df$label))
train_df_scale
train_df_scale <- transform(train_df,
CRIM = scale(train_df$CRIM, center = T, scale = T),
ZN = scale(train_df$ZN, center = T, scale = T),
INDUS = scale(train_df$INDUS, center = T, scale = T),
CHAS = scale(train_df$CHAS, center = T, scale = T),
NOX = scale(train_df$NOX, center = T, scale = T),
RM = scale(train_df$RM, center = T, scale = T),
AGE = scale(train_df$AGE, center = T, scale = T),
DIS = scale(train_df$DIS, center = T, scale = T),
RAD = scale(train_df$RAD, center = T, scale = T),
TAX = scale(train_df$TAX, center = T, scale = T),
PTRATIO = scale(train_df$PTRATIO, center = T, scale = T),
B = scale(train_df$B, center = T, scale = T),
LSTAT = scale(train_df$LSTAT, center = T, scale = T),
label = scale(train_df$label, center = T, scale = T))
train_df_scale
test_df_scale <- transform(test_df,
CRIM = scale(test_df$CRIM, center = T, scale = T),
ZN = scale(test_df$ZN, center = T, scale = T),
INDUS = scale(test_df$INDUS, center = T, scale = T),
CHAS = scale(test_df$CHAS, center = T, scale = T),
NOX = scale(test_df$NOX, center = T, scale = T),
RM = scale(test_df$RM, center = T, scale = T),
AGE = scale(test_df$AGE, center = T, scale = T),
DIS = scale(test_df$DIS, center = T, scale = T),
RAD = scale(test_df$RAD, center = T, scale = T),
TAX = scale(test_df$TAX, center = T, scale = T),
PTRATIO = scale(test_df$PTRATIO, center = T, scale = T),
B = scale(test_df$B, center = T, scale = T),
LSTAT = scale(test_df$LSTAT, center = T, scale = T),
label = scale(test_df$label, center = T, scale = T))
test_df_scale
model <- lm(label ~ ., data = train_df_scale)
summary(model)
model2 <- step(model)
dwtest(model2)  # 더빈 왓슨 값의 p-value가 0.05이상이므로 독립성 있다.
attributes(model2)
res <- residuals(model)
shapiro.test(res) # shapiro의 p-value값은 0.05이하이므로 정규분포를 따르지 않는다.
hist(res, freq = F)
qqnorm(res)
sqrt(vif(model)) > 3.3  # 다중 공선성 문제 없음
# 예측
model2_pred <- predict(model2, newdata = test_df_scale)
# 예측과 실측값 비교
plot(model2_pred, test_labels)
abline(a=0,b=1,col="blue",lty=1)
qq <- test_labels - mean(test_labels)
test_labels_nor <- qq / sd(test_labels)
# 예측과 실측값 비교
plot(model2_pred, test_labels_nor)
abline(a=0,b=1,col="blue",lty=1)
mean(abs(model2_pred - test_labels_nor))  # mae값
train_df_scale <- transform(train_df,
CRIM = scale(train_df$CRIM, center = T, scale = T),
ZN = scale(train_df$ZN, center = T, scale = T),
INDUS = scale(train_df$INDUS, center = T, scale = T),
CHAS = scale(train_df$CHAS, center = T, scale = T),
NOX = scale(train_df$NOX, center = T, scale = T),
RM = scale(train_df$RM, center = T, scale = T),
AGE = scale(train_df$AGE, center = T, scale = T),
DIS = scale(train_df$DIS, center = T, scale = T),
RAD = scale(train_df$RAD, center = T, scale = T),
TAX = scale(train_df$TAX, center = T, scale = T),
PTRATIO = scale(train_df$PTRATIO, center = T, scale = T),
B = scale(train_df$B, center = T, scale = T),
LSTAT = scale(train_df$LSTAT, center = T, scale = T),
label = train_df$label)
train_df_scale
test_df_scale <- transform(test_df,
CRIM = scale(test_df$CRIM, center = T, scale = T),
ZN = scale(test_df$ZN, center = T, scale = T),
INDUS = scale(test_df$INDUS, center = T, scale = T),
CHAS = scale(test_df$CHAS, center = T, scale = T),
NOX = scale(test_df$NOX, center = T, scale = T),
RM = scale(test_df$RM, center = T, scale = T),
AGE = scale(test_df$AGE, center = T, scale = T),
DIS = scale(test_df$DIS, center = T, scale = T),
RAD = scale(test_df$RAD, center = T, scale = T),
TAX = scale(test_df$TAX, center = T, scale = T),
PTRATIO = scale(test_df$PTRATIO, center = T, scale = T),
B = scale(test_df$B, center = T, scale = T),
LSTAT = scale(test_df$LSTAT, center = T, scale = T),
label = test_df$label)
test_df_scale
model <- lm(label ~ ., data = train_df_scale)
summary(model)
model2 <- step(model)
summary(model2)
dwtest(model2)  # 더빈 왓슨 값의 p-value가 0.05이상이므로 독립성 있다.
plot(model2, which = 1)
attributes(model2)
res <- residuals(model)
shapiro.test(res) # shapiro의 p-value값은 0.05이하이므로 정규분포를 따르지 않는다.
par(mfrow = c(1, 1))
hist(res, freq = F)
qqnorm(res)
plot(model2, which = 1)
attributes(model2)
res <- residuals(model)
shapiro.test(res) # shapiro의 p-value값은 0.05이하이므로 정규분포를 따르지 않는다.
par(mfrow = c(1, 1))
hist(res, freq = F)
qqnorm(res)
sqrt(vif(model)) > 3.3  # 다중 공선성 문제 없음
# 예측
model2_pred <- predict(model2, newdata = test_df_scale)
# 예측과 실측값 비교
plot(model2_pred, test_labels=)
# 예측과 실측값 비교
plot(model2_pred, test_labels=)
abline(a=0,b=1,col="blue",lty=1)
# 예측과 실측값 비교
plot(model2_pred, test_labels=)
# 예측과 실측값 비교
plot(model2_pred, test_labels)
abline(a=0,b=1,col="blue",lty=1)
mean(abs(model2_pred - test_labels))  # mae값
plot(model2, which = 1)
attributes(model2)
res <- residuals(model)
shapiro.test(res) # shapiro의 p-value값은 0.05이하이므로 정규분포를 따르지 않는다.
par(mfrow = c(1, 1))
hist(res, freq = F)
qqnorm(res)
train_df
view(train_df)
View(train_df)
train_df_scale <- transform(train_df,
CRIM = scale(train_df$CRIM, center = T, scale = T),
ZN = scale(train_df$ZN, center = T, scale = T),
INDUS = scale(train_df$INDUS, center = T, scale = T),
CHAS = train_df$CHAS,
NOX = scale(train_df$NOX, center = T, scale = T),
RM = scale(train_df$RM, center = T, scale = T),
AGE = scale(train_df$AGE, center = T, scale = T),
DIS = scale(train_df$DIS, center = T, scale = T),
RAD = scale(train_df$RAD, center = T, scale = T),
TAX = scale(train_df$TAX, center = T, scale = T),
PTRATIO = scale(train_df$PTRATIO, center = T, scale = T),
B = scale(train_df$B, center = T, scale = T),
LSTAT = scale(train_df$LSTAT, center = T, scale = T),
label = train_df$label)
train_df_scale
test_df_scale <- transform(test_df,
CRIM = scale(test_df$CRIM, center = T, scale = T),
ZN = scale(test_df$ZN, center = T, scale = T),
INDUS = scale(test_df$INDUS, center = T, scale = T),
CHAS = test_df$CHAS,
NOX = scale(test_df$NOX, center = T, scale = T),
RM = scale(test_df$RM, center = T, scale = T),
AGE = scale(test_df$AGE, center = T, scale = T),
DIS = scale(test_df$DIS, center = T, scale = T),
RAD = scale(test_df$RAD, center = T, scale = T),
TAX = scale(test_df$TAX, center = T, scale = T),
PTRATIO = scale(test_df$PTRATIO, center = T, scale = T),
B = scale(test_df$B, center = T, scale = T),
LSTAT = scale(test_df$LSTAT, center = T, scale = T),
label = test_df$label)
test_df_scale
model <- lm(label ~ ., data = train_df_scale)
summary(model)
model2 <- step(model)
summary(model2)
dwtest(model2)  # 더빈 왓슨 값의 p-value가 0.05이상이므로 독립성 있다.
plot(model2, which = 1)
attributes(model2)
res <- residuals(model)
shapiro.test(res) # shapiro의 p-value값은 0.05이하이므로 정규분포를 따르지 않는다.
par(mfrow = c(1, 1))
hist(res, freq = F)
qqnorm(res)
hist(res, freq = F)
qqnorm(res)
sqrt(vif(model)) > 3.3  # 다중 공선성 문제 없음
# 예측
model2_pred <- predict(model2, newdata = test_df_scale)
# 예측과 실측값 비교
plot(model2_pred, test_labels)
abline(a=0,b=1,col="blue",lty=1)
mean(abs(model2_pred - test_labels))  # mae값
train_df
train_df <- train_data %>%
as_tibble(.name_repair = "maximum") %>%
setNames(column_names) %>%
mutate(label = train_labels)
train_df <- train_data %>%
as_tibble(.name_repair = "minimul") %>%
setNames(column_names) %>%
mutate(label = train_labels)
train_df <- train_data %>%
as_tibble(.name_repair = "minimal") %>%
setNames(column_names) %>%
mutate(label = train_labels)
test_df <- test_data %>%
as_tibble(.name_repair = "minimal") %>%
setNames(column_names) %>%
mutate(label = test_labels)
train_df
test_df
# 정규화
spec <- feature_spec(train_df, label ~ . ) %>%
step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>%
fit()
spec
layer <- layer_dense_features(
feature_columns = dense_features(spec),
dtype = tf$float32
)
layer(train_df)
# 모델생성
input <- layer_input_from_dataset(train_df %>% select(-label))
output <- input %>%
layer_dense_features(dense_features(spec)) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model <- keras_model(input, output)
summary(model)
# 컴파일링
build_model <- function() {
input <- layer_input_from_dataset(train_df %>% select(-label))
output <- input %>%
layer_dense_features(dense_features(spec)) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model <- keras_model(input, output)
model %>%
compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
model
}
# 모델 훈련
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
model <- build_model()
history <- model %>% fit(
x = train_df %>% select(-label),
y = train_df$label,
epochs = 500,
validation_split = 0.2,
verbose = 0,
callbacks = list(early_stop)
)
# 시각화
plot(history)
# 모델평가
c(loss, mae) %<-% (model %>% evaluate(test_df %>% select(-label), test_df$label, verbose = 0))
paste0("Mean absolute error on test set: $", sprintf("%.2f", mae * 1000))
# 예측
test_predictions <- model %>% predict(test_df %>% select(-label))
test_predictions[ , 1]
# 예측값 실제값 비교
plot(test_predictions, test_labels)
abline(a=0,b=1,col="blue",lty=1)
# 시각화
plot(history) +
scale_x_continuous(limits = c(5, 7))
# 시각화
plot(history) +
scale_x_continuous(limits = c(0, 100))
# 시각화
plot(history) +
scale_x_continuous(limits = c(0, 80))
# 시각화
plot(history) +
scale_x_continuous(limits = c(0, 65))
qqnorm(res)
hist(res, freq = F)
boxCox(train__dff)
boxCox(train_df)
boxCox(train_df$CRIM)
boxCox(train_df$CRIM)
boxCox(train_df$CRIM~1)
train_df_scale <- transform(train_df,
CRIM = scale(train_df$CRIM, center = T, scale = T),
ZN = scale(train_df$ZN, center = T, scale = T),
INDUS = scale(train_df$INDUS, center = T, scale = T),
NOX = scale(train_df$NOX, center = T, scale = T),
RM = scale(train_df$RM, center = T, scale = T),
AGE = scale(train_df$AGE, center = T, scale = T),
DIS = scale(train_df$DIS, center = T, scale = T),
RAD = scale(train_df$RAD, center = T, scale = T),
TAX = scale(train_df$TAX, center = T, scale = T),
PTRATIO = scale(train_df$PTRATIO, center = T, scale = T),
B = scale(train_df$B, center = T, scale = T),
LSTAT = scale(train_df$LSTAT, center = T, scale = T),
label = train_df$label)
train_df_scale
test_df_scale <- transform(test_df,
CRIM = scale(test_df$CRIM, center = T, scale = T),
ZN = scale(test_df$ZN, center = T, scale = T),
INDUS = scale(test_df$INDUS, center = T, scale = T),
NOX = scale(test_df$NOX, center = T, scale = T),
RM = scale(test_df$RM, center = T, scale = T),
AGE = scale(test_df$AGE, center = T, scale = T),
DIS = scale(test_df$DIS, center = T, scale = T),
RAD = scale(test_df$RAD, center = T, scale = T),
TAX = scale(test_df$TAX, center = T, scale = T),
PTRATIO = scale(test_df$PTRATIO, center = T, scale = T),
B = scale(test_df$B, center = T, scale = T),
LSTAT = scale(test_df$LSTAT, center = T, scale = T),
label = test_df$label)
test_df_scale
model <- lm(label ~ ., data = train_df_scale)
summary(model)
model2 <- step(model)
summary(model2)
plot(model2, which = 1)
attributes(model2)
res <- residuals(model)
shapiro.test(res) # shapiro의 p-value값은 0.05이하이므로 정규분포를 따르지 않는다.
par(mfrow = c(1, 1))
hist(res, freq = F)
qqnorm(res)
library(car)
sqrt(vif(model)) > 3.3  # 다중 공선성 문제 없음
# 예측
model2_pred <- predict(model2, newdata = test_df_scale)
train_df_scale
train_df_scale <- transform(train_df,
CRIM = scale(train_df$CRIM, center = T, scale = T),
ZN = scale(train_df$ZN, center = T, scale = T),
INDUS = scale(train_df$INDUS, center = T, scale = T),
NOX = scale(train_df$NOX, center = T, scale = T),
RM = scale(train_df$RM, center = T, scale = T),
AGE = scale(train_df$AGE, center = T, scale = T),
DIS = scale(train_df$DIS, center = T, scale = T),
RAD = scale(train_df$RAD, center = T, scale = T),
TAX = scale(train_df$TAX, center = T, scale = T),
PTRATIO = scale(train_df$PTRATIO, center = T, scale = T),
B = scale(train_df$B, center = T, scale = T),
LSTAT = scale(train_df$LSTAT, center = T, scale = T),
label = train_df$label)
train_df_scale
train_df_scale[,4]
train_df_scale[,-4]
model <- lm(label ~ ., data = train_df_scale[,-4])
summary(model)
model2 <- step(model)
model <- lm(label ~ ., data = train_df_scale[,-4])
summary(model)
summary(model)
model2 <- step(model)
dwtest(model2)  # 더빈 왓슨 값의 p-value가 0.05이상이므로 독립성 있다.
plot(model2, which = 1)
attributes(model2)
res <- residuals(model)
shapiro.test(res) # shapiro의 p-value값은 0.05이하이므로 정규분포를 따르지 않는다.
par(mfrow = c(1, 1))
dwtest(model2)  # 더빈 왓슨 값의 p-value가 0.05이상이므로 독립성 있다.
hist(res, freq = F)
qqnorm(res)
sqrt(vif(model)) > 3.3  # 다중 공선성 문제 없음
# 예측
model2_pred <- predict(model2, newdata = test_df_scale[,-4])
# 예측과 실측값 비교
plot(model2_pred, test_labels[,4])
# 예측과 실측값 비교
plot(model2_pred, test_labels[,-4])
# 예측
model2_pred <- predict(model2, newdata = test_df_scale[,-4])
model2_pred
# 예측과 실측값 비교
plot(model2_pred, test_labels)
abline(a=0,b=1,col="blue",lty=1)
mean(abs(model2_pred - test_labels))  # mae값
qqnorm(res)
train_df_scale <- transform(train_df,
CRIM = log(scale(train_df$CRIM, center = T, scale = T)),
ZN = scale(train_df$ZN, center = T, scale = T),
INDUS = scale(train_df$INDUS, center = T, scale = T),
NOX = scale(train_df$NOX, center = T, scale = T),
RM = scale(train_df$RM, center = T, scale = T),
AGE = scale(train_df$AGE, center = T, scale = T),
DIS = scale(train_df$DIS, center = T, scale = T),
RAD = scale(train_df$RAD, center = T, scale = T),
TAX = scale(train_df$TAX, center = T, scale = T),
PTRATIO = scale(train_df$PTRATIO, center = T, scale = T),
B = scale(train_df$B, center = T, scale = T),
LSTAT = scale(train_df$LSTAT, center = T, scale = T),
label = train_df$label)
train_df_scale[,-4]
train_df_scale <- transform(train_df,
CRIM = log(scale(train_df$CRIM, center = T, scale = T)),
ZN = log(scale(train_df$ZN, center = T, scale = T)),
INDUS = log(scale(train_df$INDUS, center = T, scale = T)),
NOX = log(scale(train_df$NOX, center = T, scale = T)),
RM = log(scale(train_df$RM, center = T, scale = T)),
AGE = log(scale(train_df$AGE, center = T, scale = T)),
DIS = log(scale(train_df$DIS, center = T, scale = T)),
RAD = log(scale(train_df$RAD, center = T, scale = T)),
TAX = log(scale(train_df$TAX, center = T, scale = T)),
PTRATIO = log(scale(train_df$PTRATIO, center = T, scale = T)),
B = log(scale(train_df$B, center = T, scale = T)),
LSTAT = log(scale(train_df$LSTAT, center = T, scale = T)),
label = train_df$label)
train_df_scale[,-4]
train_df_scale <- transform(train_df,
CRIM = log(train_df$CRIM),
ZN = log(train_df$ZN),
INDUS = log(train_df$INDUS),
NOX = log(train_df$NOX),
RM = log(train_df$RM),
AGE = log(train_df$AGE),
DIS = log(train_df$DIS),
RAD = log(train_df$RAD),
TAX = log(train_df$TAX),
PTRATIO = log(train_df$PTRATIO),
B = log(train_df$B),
LSTAT = log(train_df$LSTAT),
label = train_df$label)
train_df_scale[,-4]
test_df_scale <- transform(test_df,
test_df_scale <- transform(test_df,
CRIM = log(test_df$CRIM),
ZN = log(test_df$ZN),
INDUS = log(test_df$INDUS),
NOX = log(test_df$NOX),
RM = log(test_df$RM),
AGE = log(test_df$AGE),
DIS = log(test_df$DIS),
RAD = log(test_df$RAD),
TAX = log(test_df$TAX),
PTRATIO = log(test_df$PTRATIO),
B = log(test_df$B),
LSTAT =log(test_df$LSTAT),
label = test_df$label)
test_df_scale
test_df_scale <- transform(test_df,
CRIM = log(test_df$CRIM),
ZN = log(test_df$ZN),
INDUS = log(test_df$INDUS),
NOX = log(test_df$NOX),
RM = log(test_df$RM),
AGE = log(test_df$AGE),
DIS = log(test_df$DIS),
RAD = log(test_df$RAD),
TAX = log(test_df$TAX),
PTRATIO = log(test_df$PTRATIO),
B = log(test_df$B),
LSTAT =log(test_df$LSTAT),
label = test_df$label)
test_df_scale
train_df_scale[,c(-2,-4)]
model <- lm(label ~ ., data = train_df_scale[,c(-2,-4)])
summary(model)
model2 <- step(model)
summary(model2)
dwtest(model2)  # 더빈 왓슨 값의 p-value가 0.05이상이므로 독립성 있다.
plot(model2, which = 1)
attributes(model2)
res <- residuals(model)
shapiro.test(res) # shapiro의 p-value값은 0.05이하이므로 정규분포를 따르지 않는다.
par(mfrow = c(1, 1))
hist(res, freq = F)
qqnorm(res)
sqrt(vif(model)) > 3.3  # 다중 공선성 문제 없음
# 예측
model2_pred <- predict(model2, newdata = test_df_scale[,-4])
# 예측
model2_pred <- predict(model2, newdata = test_df_scale[,c(-2,-4)])
# 예측
model2_pred <- predict(model2, newdata = test_df_scale[,c(-2,-4)])
# 예측과 실측값 비교
plot(model2_pred, test_labels)
abline(a=0,b=1,col="blue",lty=1)
mean(abs(model2_pred - test_labels))  # mae값
plot(model2, which = 1)
